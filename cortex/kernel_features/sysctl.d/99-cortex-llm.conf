# Cortex Linux LLM Optimization Profile
# /etc/sysctl.d/99-cortex-llm.conf
#
# Apply with: sudo sysctl -p /etc/sysctl.d/99-cortex-llm.conf
# Or reboot to apply automatically
#
# Documentation: https://github.com/cortexlinux/cortex/blob/main/docs/KERNEL_CONFIG.md

# =============================================================================
# MEMORY: Huge Pages for Model Weights
# =============================================================================
# LLMs load multi-GB weight files. Huge pages (2MB instead of 4KB) reduce
# TLB misses by 500x, improving memory access latency significantly.
#
# 4096 huge pages = 8GB reserved for models
# Adjust based on your largest model: (model_size_gb * 1024) / 2
vm.nr_hugepages = 4096

# Group ID allowed to use huge pages (default: 1000 = first user)
vm.hugetlb_shm_group = 1000

# =============================================================================
# MEMORY: Reduce Swap Pressure
# =============================================================================
# LLM inference needs weights in RAM. Swapping kills performance.
# swappiness=10 means kernel strongly prefers keeping data in RAM.
vm.swappiness = 10

# Increase dirty ratio to batch more writes (reduces I/O interrupts during inference)
vm.dirty_ratio = 40
vm.dirty_background_ratio = 10

# Don't overcommit memory - fail fast rather than OOM during inference
vm.overcommit_memory = 0
vm.overcommit_ratio = 80

# =============================================================================
# MEMORY: NUMA Configuration
# =============================================================================
# Disable automatic NUMA balancing - model weights should stay pinned
# to the NUMA node closest to the GPU
kernel.numa_balancing = 0

# =============================================================================
# MEMORY: Virtual Memory Tuning
# =============================================================================
# Increase max memory map areas (needed for large mmap'd model files)
vm.max_map_count = 2097152

# Keep more inodes/dentries in cache (helps with model file access)
vm.vfs_cache_pressure = 50

# =============================================================================
# NETWORK: Distributed Inference Optimization
# =============================================================================
# Increase buffer sizes for tensor transfer between nodes
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.core.rmem_default = 16777216
net.core.wmem_default = 16777216

# TCP buffer sizes: min, default, max
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728

# Enable TCP window scaling for high-bandwidth tensor transfer
net.ipv4.tcp_window_scaling = 1

# Reduce latency for small packets (token streaming)
net.ipv4.tcp_low_latency = 1

# =============================================================================
# KERNEL: Scheduling Hints
# =============================================================================
# Reduce scheduler migration cost (keep inference threads on same CPU)
kernel.sched_migration_cost_ns = 5000000

# Increase scheduler granularity (fewer context switches during inference)
kernel.sched_min_granularity_ns = 10000000
kernel.sched_wakeup_granularity_ns = 15000000

# =============================================================================
# KERNEL: File Descriptor Limits
# =============================================================================
# LLM servers often open many files (model shards, connections)
fs.file-max = 2097152
fs.nr_open = 2097152

# =============================================================================
# KERNEL: IPC for Multi-Process Inference
# =============================================================================
# Shared memory limits for multi-process model serving
kernel.shmmax = 68719476736
kernel.shmall = 4294967296

# Message queue limits for inter-process communication
kernel.msgmax = 65536
kernel.msgmnb = 65536
